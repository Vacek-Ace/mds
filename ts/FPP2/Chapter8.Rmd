---
title: "Exercise solutions: Section 8.11"
author: "Rob J Hyndman and George Athanasopoulos"
output:
  html_document:
    fig_height: 5
    fig_width: 8
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: false
    number_sections: false
    theme: readable
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(fpp2)
options(digits=5)
```


# Ex 1
>Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers.

> a. Explain the differences among these figures. Do they all indicate that the data are white noise?

* The figures show different critical values (blue dashed lines).

* All figures indicate that the data are white noise.

> b. Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?

* The critical values are at different distances from zero because the data sets have different number of observations. The more observations in a data set, the less noise appears in the correlation estimates (spikes). Therefore the critical values for bigger data sets can be smaller in order to check if the data is not white noise.

# Ex 2
> A classic example of a non-stationary series is the daily closing IBM stock price series (data set `ibmclose`). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced.

```{r}
ggtsdisplay(ibmclose)
```

The time plot shows the series "wandering around", which is a typical indication of non-stationarity. Differencing the series should remove this feature.

ACF does not drop quickly to zero, moreover the value $r_1$ is large and positive (almost 1 in this case). All these are signs of a non-stationary time series. Therefore it should be differenced to obtain a stationary series.

PACF value $r_1$ is almost 1. All other values $r_i, i>1$ are smaller than the critical value. These are signs of a non-stationary process that should be differenced in order to obtain a stationary series.


# Ex 3
>For the following series, find an appropriate Box-Cox transformation and order of differencing in order to obtain stationary data.
>
> a. `usnetelec`

```{r}
autoplot(usnetelec)
```

There is no need for a Box-Cox transformation in this case.

```{r}
usnetelec %>% diff() %>% autoplot()
```

> b. `enplanements`

```{r}
autoplot(enplanements)
enplanements %>% BoxCox(lambda=0) %>% diff(lag=12) %>% diff() %>% autoplot()
```

> c. `visitors`

```{r}
autoplot(visitors)
visitors %>% BoxCox(lambda=0) %>% diff(lag=12) %>% diff() %>% autoplot()
```

# Ex 4
>For the `enplanements` data, write down the differences you chose above using backshift operator notation.

A seasonal (lag 12) difference, followed by a first difference:
$$ (1-B) (1-B^{12})$$

# Ex 5

```{r ex5}
myts <- readxl::read_excel("data/retail.xlsx", skip=1)[,"A3349873A"] %>%
  ts(frequency=12, start=c(1982,4))
```

First we will try logs as before, followed by a seasonal difference.

```{r ex5a}
myts %>% log() %>% diff(lag=12) %>% ggtsdisplay()
```

That still looks a little non-stationary, so I will take another difference at lag 1.

```{r ex5b}
myts %>% log() %>% diff(lag=12) %>% diff() %>% ggtsdisplay()
```

That looks stationary now. So we used a log, a seasonal difference, and a first difference. Other time series from retail data set may require a different set of transformation/differencing choices.

# Ex 6
>Use R to simulate and plot some data from simple ARIMA models.
>  a. Use the following R code to generate data from an AR(1) model with $\phi_{1} = 0.6$ and $\sigma^2=1$. The process starts with $y_1=0$.

```{r}
ar1 <- function(phi, n=100)
{
  y <- ts(numeric(n))
  e <- rnorm(n)
  for(i in 2:n)
    y[i] <- phi*y[i-1] + e[i]
  return(y)
}
```

> b. Produce a time plot for the series. How does the plot change as you change $\phi_1$?

Some examples of changing $\phi_1$

```{r}
autoplot(ar1(0.6))
autoplot(ar1(0.95))
autoplot(ar1(0.05))
autoplot(ar1(-0.65))
```

> c. Write your own code to generate data from an MA(1) model with $\theta_{1}  =  0.6$ and $\sigma^2=1$.

```{r}
ma1 <- function(theta, n=100)
{
  y <- ts(numeric(100))
  e <- rnorm(100)
  for(i in 2:100)
    y[i] <- theta*e[i-1] + e[i]
  return(y)
}
```

> d. Produce a time plot for the series. How does the plot change as you change $\theta_1$?

```{r}
autoplot(ma1(0.6))
autoplot(ma1(0.95))
autoplot(ma1(0.05))
autoplot(ma1(-0.8))
```


> e. Generate data from an ARMA(1,1) model with $\phi_{1} = 0.6$, $\theta_{1}  = 0.6$ and $\sigma^2=1$.

```{r}
arma11 <- function(phi, theta, n=100)
{
  y <- ts(numeric(100))
  e <- rnorm(100)
  for(i in 2:100)
    y[i] <- phi*y[i-1] + theta*e[i-1] + e[i]
  return(y)
}
autoplot(arma11(0.6,0.6))
```

> f. Generate data from an AR(2) model with $\phi_{1} =-0.8$, $\phi_{2} = 0.3$ and $\sigma^2=1$. (Note that these parameters will give a non-stationary series.)

```{r}
ar2 <- function(phi1, phi2, n=100)
{
  y <- ts(numeric(100))
  e <- rnorm(100)
  for(i in 3:100)
    y[i] <- phi1*y[i-1] + phi2*y[i-2] + e[i]
  return(y)
}
autoplot(ar2(-0.8,0.3))
```

> g. Graph the latter two series and compare them.

See graphs above. The non-stationarity of the AR(2) process has led to increasing oscillations



# Ex 7

>Consider `wmurders`, the number of women murdered each year (per 100,000 standard population) in the United States.
>
> a. By studying appropriate graphs of the series in R, find an appropriate ARIMA($p,d,q$) model for these data.


```{r}
wmurders %>% ggtsdisplay()
```

The graphs suggest differencing of the data before applying ARMA models.


```{r}
wmurders %>% diff() %>% ggtsdisplay()
```

The ACF and PACF graphs suggest that the resulting time series might be an AR(2) or MA(2) due to the significant spike at lag 2 in both graphs. So either an ARIMA(0,1,2) or an ARIMA(2,1,0) appear appropriate. As we need to make a choice, I'll use an ARIMA(0,1,2) in what follows.

> b. Should you include a constant in the model? Explain.

A constant would imply a drift in the original data which does not look correct, so we omit the constant.

> c. Write this model in terms of the backshift operator.

ARIMA(0,1,2): $(1-B)y_t = (1 + \theta_1B + \theta_2B^2) \varepsilon_{t}$.

> d. Fit the model using R and examine the residuals. Is the model satisfactory?


```{r}
(fit <- Arima(wmurders, c(0,1,2)))
checkresiduals(fit)
```

* No obvious patterns are visible on the graph of the residuals
* Ljung-Box test suggests that the residuals are white noise
* The residuals look close enough to normally distributed

The model is satisfactory.


> e. Forecast three times ahead. Check your forecasts by hand to make sure that you know how they have been calculated.

```{r}
(fcast <- forecast(fit, h = 3))
```

```{r echo=FALSE}
theta1 <- coefficients(fit)[1]
theta2 <- coefficients(fit)[2]
fc <- fcast$mean
# Store rounded coefficients for equations
th1 <- format(theta1,nsmall=3,digits=3)
th2 <- format(theta2,nsmall=3,digits=3)
# Store residuals
res <- residuals(fcast)
```

To check the forecasts by hand we use the model written in terms of the backshift operator:
$$
 (1-B)y_t = (1 + \theta_1B + \theta_2B^2) \varepsilon_{t},
$$
where $y_t$ is the observed time series and the coefficients are
$\theta_1 = `r th1`$ and $\theta_2 = `r th2`$. It can be rewritten as:
$$
 y_t = y_{t-1} + \varepsilon_t `r th1`\varepsilon_{t-1} + `r th2`\varepsilon_{t-2}.
$$
The last observation is `r round(tail(wmurders,1),4)`
and the last few residuals are `r tail(c(res), 3)`. So the next forecast is:
$$
  \hat{y}_{T+1|T} = `r round(tail(wmurders,1),4)` + 0
    `r th1` (`r round(tail(res,1),4)`) +
    `r th2` (`r round(tail(res,2)[1],4)`) = `r round(fc[1],4)`.
$$
Similarly,
$$
  \hat{y}_{T+2|T} = `r round(fc[1],4)` + 0 + 0 +
    `r th2` (`r round(tail(res,1),4)`) = `r round(fc[2],4)`
$$
and
$$
  \hat{y}_{T+3|T} = `r round(fc[2],4)` + 0 + 0 + 0 = `r round(fc[3],4)`.
$$

> f. Create a plot of the series with forecasts and prediction intervals for the next three periods shown.

```{r}
autoplot(fcast)
```

> g. Does `auto.arima` give the same model you have chosen? If not, which model do you think is better?

```{r}
fit.auto <- auto.arima(wmurders)
fcast.auto <- forecast(fit.auto, h = 3)
autoplot(fcast.auto)
```

`auto.arima` picked a more complex model which assumes that the trend in the data will continue in the future. The extra level of differencing is hard to justify from the graphs, and it is unlikely that murder rates will fall indefinitely. So I'd probably prefer only one difference. We can force `auto.arima` to do that:

```{r}
(fit.auto = auto.arima(wmurders, d=1, stepwise=FALSE))
```

Now `auto.arima` picks the same model that I did.


# Ex 8
> Consider `austa`, the total international visitors to Australia (in millions) for the period 1980-2015.

>  a. Use `auto.arima()` to find an appropriate ARIMA model. What model was selected. Check that the residuals look like white noise. Plot forecasts for the next 10 periods.

```{r}
(fit <- auto.arima(austa))
```

An `r as.character(fit)` model is selected .

```{r}
checkresiduals(fit)
```

The residuals look like white noise.

```{r}
fit %>% forecast(h=10) %>% autoplot()
```

>  b. Plot forecasts from an ARIMA(0,1,1) model with no drift and compare these to part a. Remove the MA term and plot again.

```{r}
austa %>% Arima(order=c(0,1,1), include.constant=FALSE) %>%
  forecast() %>% autoplot()
```

```{r}
austa %>% Arima(order=c(0,1,0), include.constant=FALSE) %>%
  forecast() %>% autoplot()
```

Removing the drift term makes a big difference as the forecasts no longer have a linear trend. Removing the MA(1) term makes almost no difference.

>  c. Plot forecasts from an ARIMA(2,1,3) model with drift. Remove the constant and see what happens.

```{r}
austa %>% Arima(order=c(2,1,3), include.constant=TRUE) %>%
  forecast() %>% autoplot()
```

```r
austa %>% Arima(order=c(2,1,3), include.constant=FALSE) %>%
  forecast() %>% autoplot()
```

```
Error in stats::arima(x = x, order = order, seasonal = seasonal, include.mean = include.mean,  :
  non-stationary AR part from CSS
```

The model fitting causes an error because the drift is really needed. What happens is that the AR terms try to handle the drift but end up going outside the stationarity region.

>  d. Plot forecasts from an ARIMA(0,0,1) model with a constant. Remove the MA term and plot again.

```{r}
austa %>% Arima(order=c(0,0,1), include.constant=TRUE) %>%
  forecast() %>% autoplot()
```

```{r}
austa %>% Arima(order=c(0,0,1), include.constant=FALSE) %>%
  forecast() %>% autoplot()
```

A stationary model with a constant has long term forecasts equal to the mean (long term here meaning after the first horizon). A stationary model without a constant has long term forecasts equal to zero.


>  e. Plot forecasts from an ARIMA(0,2,1) model with no constant.

```{r}
austa %>% Arima(order=c(0,2,1), include.constant=FALSE) %>%
  forecast() %>% autoplot()
```

The second level of differencing induces a linear trend in the forecasts, but the prediction intervals become quite wide.


# Ex 9

>For the `usgdp` series:
> a. if necessary, find a suitable Box-Cox transformation for the data;

```{r}
autoplot(usgdp)
```

I don't think a Box-Cox transformation is required.


> b. fit a suitable ARIMA model to the transformed data using `auto.arima()`;

```{r}
(fit <- auto.arima(usgdp))
```

> c. try some other plausible models by experimenting with the orders chosen;

The second order differencing will induce a trend in the forecasts, which is required here, so I will look at changing only $p$ and $q$.

```{r}
fit020 <- Arima(usgdp, order=c(0,2,0))
fit021 <- Arima(usgdp, order=c(0,2,1))
fit022 <- Arima(usgdp, order=c(0,2,2))
fit023 <- Arima(usgdp, order=c(0,2,3))
fit120 <- Arima(usgdp, order=c(1,2,0))
fit121 <- Arima(usgdp, order=c(1,2,1))
fit122 <- Arima(usgdp, order=c(1,2,2))
fit123 <- Arima(usgdp, order=c(1,2,3))
fit220 <- Arima(usgdp, order=c(2,2,0))
fit221 <- Arima(usgdp, order=c(2,2,1))
fit222 <- Arima(usgdp, order=c(2,2,2))
fit223 <- Arima(usgdp, order=c(2,2,3))
fit320 <- Arima(usgdp, order=c(3,2,0))
fit321 <- Arima(usgdp, order=c(3,2,1))
fit322 <- Arima(usgdp, order=c(3,2,2))
fit323 <- Arima(usgdp, order=c(3,2,3))
```

> d. choose what you think is the best model and check the residual diagnostics;

```{r echo=FALSE}
best <- which.min(c(
  fit020$aicc,
  fit021$aicc,
  fit022$aicc,
  fit023$aicc,
  fit120$aicc,
  fit121$aicc,
  fit122$aicc,
  fit123$aicc,
  fit220$aicc,
  fit221$aicc,
  fit222$aicc,
  fit223$aicc,
  fit320$aicc,
  fit321$aicc,
  fit322$aicc,
  fit322$aicc
  ))
if(best != 10L)
  stop("Wrong model")
```
The best according to the AICc values is the ARIMA(2,2,1) model.

```{r}
checkresiduals(fit221)
```

The residuals pass the Ljung-Box test, but the histogram looks like it has heavier tails than Gaussian.

> e. produce forecasts of your fitted model. Do the forecasts look reasonable?


```{r}
fit221 %>% forecast(h=20) %>% autoplot
```

These look reasonable.

> f. compare the results with what you would obtain using `ets()` (with no transformation).

```{r}
usgdp %>% ets %>% forecast(h=20) %>% autoplot
```

The ETS point forecasts are higher than the ARIMA forecasts, and the ETS forecast intervals are wider.

# Ex 10

> Consider `austourists`, the quarterly number of international tourists to Australia for the period 1999--2010.

> a. Describe the time plot.

```{r}
autoplot(austourists)
```

```{r}
lambda = BoxCox.lambda(austourists)
```

BoxCox.lambda function suggests using Box-Cox transformation with parameter $\lambda=`r round(lambda,3)`$. This value is very close to $0$ and moreover Box-Cox transformation with parameter $0$ has another meaning: it is log transformation which ensures that the forecasted values transformed back to the original scale (using exponent transformation) will be positive. Since number of tourists visiting Australia cannot be negative it makes sense. Therefore we will just use logs of the original data.

```{r}
austourists %>% log() %>% ggtsdisplay()
```

The ACF and PACF graphs suggest seasonal differencing of the data before applying ARMA models.

```{r}
tsdisplay(diff(log(austourists), lag=4))
```

The ACF and PACF graphs suggest that the resulting time series can be ARMA(0,1)(0,1) or ARMA(0,1)(1,0) or ARMA(1,0)(0,1) or ARMA(1,0)(1,0) models.
Therefore the suggested ARIMA model for log(austourists) data are ARIMA(0,0,1)(0,1,1) or ARIMA(0,0,1)(1,1,0) or ARIMA(1,0,0)(0,1,1) or ARIMA(1,0,0)(1,1,0).
A constant should be included in the model since the differenced time series is raised significantly above zero. The constant will lead to a linear trend in the forecasts. Such behaviour is strongly supported by the data.

```{r}
fit1 = Arima(austourists, c(0,0,1), c(0,1,1), include.drift = TRUE, lambda = 0)
fit2 = Arima(austourists, c(0,0,1), c(1,1,0), include.drift = TRUE, lambda = 0)
fit3 = Arima(austourists, c(1,0,0), c(0,1,1), include.drift = TRUE, lambda = 0)
fit4 = Arima(austourists, c(1,0,0), c(1,1,0), include.drift = TRUE, lambda = 0)
```

The best model out of these four can be chosen using AICc:

```{r}
c(fit1$aicc, fit2$aicc, fit3$aicc, fit4$aicc)
```
The lowest AICc is for model ARIMA(1,0,0)(0,1,1) with drift, therefore it is the preferred model for forecasting.

```{r}
auto.arima(austourists, lambda = 0)
```

auto.arima finds the same model according to AICc.

The best model written using backshift operator and without it

Using backshift operator the model can be written as:
$(1 - \phi_1 B) (1 - B^4) y_t = c + (1 + \theta_1 B^4) \epsilon_t$

Without the backshift operator the model can be written as:
$y_t  - \phi_1 y_{t-1} - y_{t-4} + \phi_1 y_{t-5} = c + \epsilon_t + \theta_1 \epsilon_{t-4}$
or:
$y_t = \phi_1 y_{t-1} + y_{t-4} - \phi_1 y_{t-5} + c + \epsilon_t + \theta_1 \epsilon_{t-4}$

The last form allows forecasting value $y_t$ assuming that values $y_{t-1}$, $y_{t-4}$, $y_{t-5}$ and $\epsilon_{t-4}$ are known. Value $\epsilon_t$, although, should be estimated as zero.


> b. What can you learn from the ACF graph?

> c. What can you learn from the PACF graph?

> d. Produce plots of the seasonally differenced data $(1 - B^{4})Y_{t}$.  What model do these graphs suggest?

> e. Does `auto.arima()` give the same model that you chose? If not, which model do you think is better?

> f. Write the model in terms of the backshift operator, then without using the backshift operator.

# Ex 11
> Consider `usmelec`, the total net generation of electricity (in billion kilowatt hours) by the U.S. electric industry (monthly for the period January 1973 -- June 2013). In general there are two peaks per year: in mid-summer and mid-winter.

> a. Examine the 12-month moving average of this series to see what kind of trend is involved.

> b. Do the data need transforming? If so, find a suitable transformation.

> c. Are the data stationary? If not, find an appropriate differencing which yields stationary data.

> d. Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AIC values?

> e. Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.

> f. Forecast the next 15 years of electricity generation by the U.S. electric industry. Get the latest figures from [the EIA](https://bit.ly/usmelec) to check the accuracy of your forecasts.

> g. Eventually, the prediction intervals are so wide that the forecasts are not particularly useful. How many years of forecasts do you think are sufficiently accurate to be usable?


### Data set usmelec

```{r}
usmelec
plot(usmelec); lines(ma(usmelec, 12), col = "red", lwd = 2)
```

The data show strong up-trend which might be considered as linear in long run.

The seasonal pattern which size increases together with the level of the trend strongly suggests using Box-Cox transformation before forecasting.

```{r}
lambda = BoxCox.lambda(usmelec)
plot(BoxCox(usmelec, lambda))
```

BoxCox.lambda function suggests using Box-Cox transformation with parameter $\lambda=`r round(lambda,2)`$.

The data is not stationary (it contains trend), therefore differencing is required to make it stationary for ARMA forecasting.

```{r}
tsdisplay(diff(BoxCox(usmelec, lambda), lag = 12))
```

ACF and PACF graphs suggest that the data is stationary. Suggested models are:
ARIMA(1,0,0)(0,1,1)
ARIMA(2,0,0)(0,1,1)
ARIMA(3,0,0)(0,1,1)

```{r}
fit1 = Arima(usmelec, c(1,0,0), c(0,1,1), lambda = lambda)
fit2 = Arima(usmelec, c(2,0,0), c(0,1,1), lambda = lambda)
fit3 = Arima(usmelec, c(3,0,0), c(0,1,1), lambda = lambda)
```

```{r}
c(fit1$aicc, fit2$aicc, fit3$aicc)
```
The lowest AICc is for model ARIMA(3,0,0)(0,1,1).

#### Residuals analysis

```{r}
tsdisplay(fit3$residuals)
```

```{r}
Box.test(fit3$residuals, lag = 5, type = "Ljung-Box", fitdf = 4)
```

ACF and PACF graphs as well as Ljung-Box test suggest that the residuals are not white noise.
Looking at the residuals we can suggest spikes at lags 3 and 4 at ACF graph can be removed by increasing MA value in ARIMA model:

```{r}
fit4 = Arima(usmelec, c(3,0,1), c(0,1,1), lambda = lambda)
fit5 = Arima(usmelec, c(3,0,2), c(0,1,1), lambda = lambda)
fit6 = Arima(usmelec, c(3,0,3), c(0,1,1), lambda = lambda)
```

```{r}
c(fit4$aicc, fit5$aicc, fit6$aicc)
```

It appears that currently model ARIMA(3,0,1)(0,1,1) is with the lowest AICc.

The residuals for this model are:

```{r}
tsdisplay(fit4$residuals)
```

Which are good except for the spikes at ACF and PACF graphs at lag 15. We will try to rectify it by increasing AR and MA values for seasonal and non-seasonal ARIMA parts:

```{r}
fit7 = Arima(usmelec, c(3,0,1), c(1,1,1), lambda = lambda)
fit8 = Arima(usmelec, c(4,0,1), c(0,1,1), lambda = lambda)
fit9 = Arima(usmelec, c(4,0,1), c(1,1,1), lambda = lambda)
```

```{r}
c(fit7$aicc, fit8$aicc, fit9$aicc)
```

All received AICc values are greater than for the best found model ARIMA(3,0,1)(0,1,1). Therefore ARIMA(3,0,1)(0,1,1) should be used for forecasting.
The residuals although appear to be correlated for lag 15:

```{r}
Box.test(fit4$residuals, lag = 15, type = "Ljung-Box", fitdf = 4)
```

It means that the forecasting intervals, calculated using this model can be incorrect.

### Forecasting 15 years ahead and comparing with existing data (from http://www.eia.gov/totalenergy/data/monthly/#electricity)

```{r}
# New data taken from http://www.eia.gov/totalenergy/data/monthly/#electricity website
newData = c(306009.629,362119.081,362871.893,313126.607,318709.941,302400.724,323628.238,
         367727.015,418692.755,406511.315,337931.318,308698.504,304102.155,335740.463,
         339528.259,309389.42,309090.569,295228.216,336517.631,360826.151,414639.671,
         395699.566,334584.877,311650.774,305975.035,334635.089,348489.737,309435.119,
         325301.486,298073.696,321833.617,356224.262,393799.21,383968.062,340293.27,
         314682.729,313751.509,352356.534,377019.193,323662.011,331595.192,296766.045,
         323730.672,357419.408,384838.868,383494.071,338975.899,313971.928,317176.051)/1000
newTs = ts(newData, start = c(2010, 11), frequency = 12)

fcast = forecast(fit4, h = 15*12)
plot(fcast)
lines(newTs, col = "red", lwd = 1.5)
accuracy(fcast, newTs)
```

The forecasts are good. Probably quite a few years of forecasts can be correct. Although the forecasting intervals are expanding very quickly and therefore the forecasts become less and less useful when the forecasting horizon increases.


# Ex 12 
> For the `mcopper` data:

> a. if necessary, find a suitable Box-Cox transformation for the data;

> b. fit a suitable ARIMA model to the transformed data using `auto.arima()`;

> c. try some other plausible models by experimenting with the orders chosen;

> d. choose what you think is the best model and check the residual diagnostics;

> e. produce forecasts of your fitted model. Do the forecasts look reasonable?

> f. compare the results with what you would obtain using `ets()` (with no transformation).

### Data set mcopper

```{r}
mcopper
plot(mcopper)
```

```{r}
lambda = BoxCox.lambda(mcopper)
plot(BoxCox(mcopper, lambda))
```

BoxCox.lambda function suggests using Box-Cox transformation with parameter $\lambda=`r round(lambda,2)`$.

### Finding a suitable model using auto.arima and trying other models

```{r}
fit = auto.arima(mcopper, lambda = lambda)
fit
```

Other models which can be  tried are ARIMA(2,1,0) and ARIMA(7,1,0):

```{r}
fit1 = Arima(mcopper, c(2,1,0), lambda = lambda)
fit2 = Arima(mcopper, c(7,1,0), lambda = lambda)
```

```{r}
c(fit$aicc, fit1$aicc, fit2$aicc)
```

The lowest AICc is for model ARIMA(0,1,1) ( the model which was proposed by auto.arima).

### Residuals diagnostics for model ARIMA(0,1,1)

```{r}
hist(fit$residuals)
tsdisplay(fit$residuals)
Box.test(fit$residuals, lag = 12, type = "Ljung-Box", fitdf = 1)
```

* The residuals look normal at the histogram

* No obvious patterns are visible on the graph of the residuals

* The residuals are white noise

### Comparing forecasts of auto.arima and ets

```{r}
fcast = forecast(fit, h = 20)
plot(fcast)
```

```{r}
fit.ets = ets(mcopper)
fcast.ets = forecast(fit.ets, h = 20)
plot(fcast.ets)
```

Forecasts look differently, but taking into account that the forecasting intervals are very wide, both forecasts provide similar results.


# Ex 13
> Choose one of the following seasonal time series: `hsales`, `auscafe`, `qauselec`, `qcement`, `qgas`.

> a. Do the data need transforming? If so, find a suitable transformation.

> b. Are the data stationary? If not, find an appropriate differencing which yields stationary data.

> c. Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AIC values?

> d. Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.

> e. Forecast the next 24 months of data using your preferred model.

> f. Compare the forecasts obtained using `ets()`.

# Ex 14
> For the same time series you used in the previous exercise, try using a non-seasonal model applied to the seasonally adjusted data obtained from STL. The `stlf()` function will make the calculations easy (with `method="arima"`). Compare the forecasts with those obtained in the previous exercise. Which do you think is the best approach?

# Ex 15 
> For your retail time series (Exercise 5 above):

> a. develop an appropriate seasonal ARIMA model;

> b. compare the forecasts with those you obtained in earlier chapters;

> c. Obtain up-to-date retail data from the [ABS website](https://bit.ly/absretail) (Cat 8501.0, Table 11), and compare your forecasts with the actual numbers. How good were the forecasts from the various models?

# Ex 16
> a. Produce a time plot of the sheep population of England and Wales from 1867--1939 (data set `sheep`).

```{r}
autoplot(sheep)
```

> b. Assume you decide to fit the following model:
$$
  y_t = y_{t-1} + \phi_1(y_{t-1}-y_{t-2}) + \phi_2(y_{t-2}-y_{t-3}) + \phi_3(y_{t-3}-y_{t-4}) + e_t,
$$
where $e_t$ is a white noise series. What sort of ARIMA model is this (i.e., what are $p$, $d$, and $q$)?

ARIMA(3,1,0). $p=3$, $d=1$, $q=0$.

> c. By examining the ACF and PACF of the differenced data, explain why this model is appropriate.

```{r}
sheep %>% diff() %>% ggtsdisplay()
```

The last significant spike in the PACF is at lag 3, so $p=3$ is a good choice. Hence an ARIMA(3,1,0) model.

```{r sheepfit, echo=FALSE, cache=TRUE}
fit <- Arima(sheep, order=c(3,1,0))
phi1 <- coef(fit)['ar1']
phi2 <- coef(fit)['ar2']
phi3 <- coef(fit)['ar3']
```

> d. The last five values of the series are given below:
>
>|Year              | 1935| 1936| 1937| 1938| 1939|
>|:-----------------|----:|----:|----:|----:|----:|
>|Millions of sheep | 1648| 1665| 1627| 1791| 1797|
>
>The estimated parameters are
>$\phi_1 = `r format(phi1, digits=2, nsmall=2)`$,
>$\phi_2 = `r format(phi2, digits=2, nsmall=2)`$, and
>$\phi_3 = `r format(phi3, digits=2, nsmall=2)`$.
>Without using the `forecast` function, calculate forecasts for the next three years (1940--1942).

From the equation above
\begin{align*}
\hat{y}_{T+1|T}
  &= y_{T} + \phi_1(y_{T}-y_{T-1}) + \phi_2(y_{T-1}-y_{T-2}) + \phi_3(y_{T-2}-y_{T-3})\\
  &= 1797 + `r format(phi1, digits=2, nsmall=2)` (1797-1791)
          + `r format(phi2, digits=2, nsmall=2)` (1791-1627)
          + `r format(phi3, digits=2, nsmall=2)` (1627-1665) \\
  &= `r format(f1 <- 1797 + phi1*(1797-1791) + phi2* (1791-1627) + phi3* (1627-1665), digits=2, nsmall=2)` \\
\hat{y}_{T+2|T}
  &= \hat{y}_{T+1|T} + \phi_1(\hat{y}_{T+1|T}-y_{T}) + \phi_2(y_{T}-y_{T-1}) + \phi_3(y_{T-1}-y_{T-2})\\
  &= `r f1`  + `r format(phi1, digits=2, nsmall=2)` (`r f1` -1797)
          + `r format(phi2, digits=2, nsmall=2)` (1797-1791)
          + `r format(phi3, digits=2, nsmall=2)` (1791-1627) \\
  &= `r format(f2 <- f1 + phi1*(f1-1797) + phi2* (1797-1791) + phi3* (1791-1627), digits=2, nsmall=2)` \\
\hat{y}_{T+3|T}
  &= \hat{y}_{T+2|T} + \phi_1(\hat{y}_{T+2|T}-\hat{y}_{T+1|T}) + \phi_2(\hat{y}_{T+1|T}-y_{T}) + \phi_3(y_{T}-y_{T-1})\\
  &= `r f2`  + `r format(phi1, digits=2, nsmall=2)` (`r f2` - `r f1`)
          + `r format(phi2, digits=2, nsmall=2)` (`r f1`-1797)
          + `r format(phi3, digits=2, nsmall=2)` (1797-1791) \\
  &= `r format(f3 <- f2 + phi1*(f2-f1) + phi2* (f1-1797) + phi3* (1797-1791), digits=2, nsmall=2)`
\end{align*}


> e. Now fit the model in R and obtain the forecasts using `forecast`. How are they different from yours? Why?

```{r}
forecast(fit, h=3)
```

They are the same. If you had different values, it is most likely due to rounding errors.


# Ex 17

> a. Plot the annual bituminous coal production in the United States from 1920 to 1968 (data set \verb|bicoal|).

```{r}
autoplot(bicoal)
```

> b. You decide to fit the following model to the series:
$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \phi_3 y_{t-3} + \phi_4 y_{t-4} + e_t$$
where $y_t$ is the coal production in year $t$ and $e_t$ is a white noise series.
What sort of ARIMA model is this (i.e., what are $p$, $d$, and $q$)?

ARIMA(4,0,0). $p=4$, $d=0$, $q=0$.

> c. Explain why this model was chosen using the ACF and PACF.

```{r}
ggtsdisplay(bicoal)
```

The PACF has a signifcant spike at lag 4, but none at higher lags.


> d. The last five values of the series are given below.
>
>|Year              | 1964| 1965| 1966| 1967| 1968|
>|:-----------------|----:|----:|----:|----:|----:|
>|Millions of tons  | 467 | 512 | 534 | 552 | 545 |

```{r bicoalfit, echo=FALSE, cache=TRUE}
fit <- Arima(bicoal, order=c(4,0,0))
mu <- coef(fit)['intercept']
phi1 <- coef(fit)['ar1']
phi2 <- coef(fit)['ar2']
phi3 <- coef(fit)['ar3']
phi4 <- coef(fit)['ar4']
intercept <- mu * (1-phi1-phi2-phi3-phi4)
# Store rounded versions for printing
c <- format(intercept, digits=2, nsmall=2)
p1 <- format(phi1, digits=2, nsmall=2)
p2 <- format(phi2, digits=2, nsmall=2)
p3 <- format(phi3, digits=2, nsmall=2)
p4 <- format(phi4, digits=2, nsmall=2)
```

> The estimated parameters are
 $c = `r c`$,
 $\phi_1 = `r p1`$,
 $\phi_2 = `r p2`$,
 $\phi_3 = `r p3`$, and
 $\phi_4 = `r p4`$.
 Without using the `forecast` function, calculate forecasts for the next three years (1969--1971).

\begin{align*}
\hat{y}_{T+1|T}
  &= c + \phi_1 y_{T} + \phi_2 y_{T-1} + \phi_3 y_{T-2} + \phi_4 y_{T-3} \\
  &= `r c` + `r p1` *545  `r p2` *552 + `r p3` *534  `r p4` *512 \\
  &= `r format(f1 <- intercept + phi1*545 + phi2*552 + phi3*534 + phi4*512, nsmall=2)` \\
\hat{y}_{T+2|T}
  &= c + \phi_1 \hat{y}_{T+1|T} + \phi_2 y_{T} + \phi_3 y_{T-1} + \phi_4 y_{T-2} \\
  &= `r c` + `r p1`* `r format(f1,nsmall=2)`  `r p2` *545 + `r p3` *552  `r p4`* 534 \\
  &= `r format(f2 <- intercept + phi1*f1 + phi2*545 + phi3*552 + phi4*534, nsmall=2)` \\
\hat{y}_{T+3|T}
  &= c + \phi_1 \hat{y}_{T+2|T} + \phi_2 \hat{y}_{T+1|T} + \phi_3 y_{T} + \phi_4 y_{T-1} \\
  &= `r c` + `r p1`* `r format(f2,nsmall=2)`  `r p2` * `r format(f1,nsmall=2)` + `r p3` *545  `r p4`* 552 \\
  &= `r format(intercept + phi1*f2 + phi2*f1 + phi3*545 + phi4*552, nsmall=2)` \\
\end{align*}


> b. Now fit the model in R and obtain the forecasts from the same model. How are they different from yours? Why?

```{r}
forecast(fit, h=3)
```

Again, any differences between these values and those computed "by hand" are probably due to rounding errors.

# Ex 18



